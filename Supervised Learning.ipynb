{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "# Model Building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import zscore\n",
    "%matplotlib inline\n",
    "\n",
    "# loading library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # calculate accuracy measures and confusion matrix\n",
    "from sklearn.metrics import confusion_matrix # Creating  a confusion matrix,which compares the y_test and y_pred\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1.A\n",
    "Normal = pd.read_csv('Normal.csv') #reading the dataset\n",
    "Type_H = pd.read_csv('Type_H.csv') #reading the dataset\n",
    "Type_S = pd.read_csv('Type_S.csv') #reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.B\n",
    "#Normal Dataframe\n",
    "print ('Shape of Normal Dataframe:',Normal.shape)\n",
    "print ('Columns of Normal Dataframe:',Normal.columns)\n",
    "\n",
    "#Type_H Dataframe\n",
    "print ('Shape of Type_H Dataframe:',Type_H.shape)\n",
    "print ('Columns of Type_H Dataframe:',Type_H.columns)\n",
    "\n",
    "#Type_S Dataframe\n",
    "print ('Shape of Type_S Dataframe:',Type_S.shape)\n",
    "print ('Columns of Type_S Dataframe:',Type_S.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.C\n",
    "print('Data Types of Normal Dataframe: ',Normal.dtypes)  # To get the data types of all the columns in dataframe\n",
    "print('Data Types of Type_H Dataframe: ',Type_H.dtypes)\n",
    "print('Data Types of Type_S Dataframe: ',Type_S.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.C Observation\n",
    "    1. Basis above observation/results we can say that all the three dataframes have same column names\n",
    "    2. The data types of the column name is also same across all the three dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1.D\n",
    "print('Data Types of Normal Dataframe: ',Normal.dtypes)  # To get the data types of all the columns in dataframe\n",
    "print('Data Types of Type_H Dataframe: ',Type_H.dtypes)\n",
    "print('Data Types of Type_S Dataframe: ',Type_S.dtypes)\n",
    "\n",
    "#Alternative\n",
    "print('Data Types of Normal Dataframe: ',Normal.info())  # To get the data types of all the columns in dataframe\n",
    "print('Data Types of Type_H Dataframe: ',Type_H.info())\n",
    "print('Data Types of Type_S Dataframe: ',Type_S.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.E\n",
    "print(Normal['Class'].unique())\n",
    "print(Type_H['Class'].unique())\n",
    "print(Type_S['Class'].unique())\n",
    "\n",
    "print(Normal['Class'].value_counts())\n",
    "print(Type_H['Class'].value_counts())\n",
    "print(Type_S['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis above results, we can say that Normal Dataframe contains these '['Normal' 'Nrmal']'classes while Type_H contains these'['Type_H' 'type_h']' classes and Type_S contains these'['Type_S' 'tp_s']'classes. The three dataframe consist of three differnt types of people.\n",
    "\n",
    "Classes entry can be unified later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.A\n",
    "# this will replace \"Nrmal\" with \"Normal\"\n",
    "Normal = Normal.replace(to_replace =\"Nrmal\",value =\"Normal\")\n",
    "# this will replace \"type_h\" with \"Type_H\"\n",
    "Type_H = Type_H.replace(to_replace =\"type_h\",value =\"Type_H\")\n",
    "# this will replace \"tp_s\" with \"Type_S\"\n",
    "Type_S = Type_S.replace(to_replace =\"tp_s\",value =\"Type_S\")\n",
    "\n",
    "print(Normal['Class'].unique())\n",
    "print(Type_H['Class'].unique())\n",
    "print(Type_S['Class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only one type of class exist for each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q2.B\n",
    "dataframes = [Normal, Type_H, Type_S]\n",
    "\n",
    "Medical = pd.concat(dataframes) #combine dataframes through Concat method\n",
    "print('Shape of new dataframe after combining',Medical.shape)\n",
    "\n",
    "print(Medical['Class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.C\n",
    "Medical.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q2.D\n",
    "#Count of missing for each feature\n",
    "print(Medical.isnull().sum())\n",
    "#to get percentage of missing\n",
    "print(Medical.isnull().sum() * 100 / len(Medical))\n",
    "\n",
    "# to check if any other ?/NAN present \n",
    "for i in Medical.columns:\n",
    "    print(Medical[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values for Medical Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.E\n",
    "Medical.describe() # to get Min,Max median, Q1 and Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q3.A\n",
    "Medical_1 = Medical[[\"P_incidence\",\"P_tilt\",\"L_angle\",\"S_slope\",\"P_radius\",\"S_Degree\"]] #to remove target variable\n",
    "# ax = sns.heatmap(Medical_1, cmap=\"YlGnBu\")\n",
    "grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .3}\n",
    "f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws)\n",
    "ax = sns.heatmap(Medical_1, ax=ax, cmap=\"YlGnBu\",\n",
    "                 cbar_ax=cbar_ax,\n",
    "                 cbar_kws={\"orientation\": \"horizontal\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is difficult to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mask the above triangle \n",
    "corr = Medical_1.corr()\n",
    "masking = np.zeros_like(corr)\n",
    "masking[np.triu_indices_from(masking)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax = sns.heatmap(corr, mask=masking, cmap=\"YlGnBu\",vmax=.5, square=True, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.B\n",
    "print(Medical.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Features having stronger correlation with correlation value.\n",
    "\n",
    "##### P_incidence is highly correlated with all the remaning features except P_radius\n",
    "    1. P_incidence, S_slope = 0.814960 (very strong)\n",
    "    2. P_incidence, L_angle = 0.717282 (strong)\n",
    "    3. P_incidence, S_Degree = 0.638743\n",
    "    4. P_incidence, P_tilt = 0.629199\n",
    "    5. S_slope, L_angle = 0.598387\n",
    "\n",
    "\n",
    "#### B. Features having weaker correlation with correlation value\n",
    "\n",
    "    1. S_Degree and P_radius = -0.026065\n",
    "    2. P_radius and L_angle = -0.080344 \n",
    "    3. P_radius and P_tilt = 0.032668\n",
    "    4. P_tilt and S_slope = 0.062345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Medical.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cannot reindex from a duplicate axis : to remove this error IN FUTURE\n",
    "Medical.reset_index(inplace=True)\n",
    "Medical[Medical.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Medical = Medical.drop(labels= \"index\" , axis = 1)\n",
    "Medical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.C\n",
    "sns.pairplot(Medical, hue=\"Class\",diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Insights:  \n",
    "    1. L_angle, S_slope and P_tilt are positively related to P_incidence\n",
    "    2. L_angle and S_slope are also positively related\n",
    "    3. S_Degree is somewhat positively skewed\n",
    "    4. Type_S class genrally have higher value for all the features except P_Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.D\n",
    "sns.jointplot(data=Medical, x=\"P_incidence\", y=\"S_slope\", kind=\"reg\")\n",
    "sns.jointplot(data=Medical, x=\"P_incidence\", y=\"S_slope\", hue=\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. For most of the Type_S class, P_incidence is high compared to Normal and Type_H clas i.e., Type_S has large  values for P_incidence compared to other two.\n",
    "    2. Also,P_incidence and S_slope is positively correlated for all the three class of people\n",
    "    3. P_incidence is little negatively skewed for Type_S Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.E\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.ylim(-20,200,80)\n",
    "ax = sns.boxplot(data=Medical,palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. P_incidence is normally distributed with very few outliers\n",
    "    2. P_tilt is also normally distributed with few outliers but more than P_incidence. It's value goes in negative for few cases.\n",
    "    3. L_angle is little positively skewed with one outlier\n",
    "    4. S_Slope is very little positively skewed with one outlier\n",
    "    5. P_radius is normally distributed with outliers on both the sides of whiskers. Also, it's value are comparitively higher than other features which hints for scaling.\n",
    "    6. S_Degree is positively skewed with outliers on the positive side. It's value also goes in negative for few cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Medical.Class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to label encode the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_label_encoder = LabelEncoder()\n",
    "\n",
    "Medical.iloc[:,-1] = class_label_encoder.fit_transform(Medical.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Medical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Medical['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Medical['Class'] = Medical.Class.astype('category')\n",
    "# Medical.info\n",
    "Medical.groupby([\"Class\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Medical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Medical.var() #to check variance in features\n",
    "#Insight: Good variance among independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.A\n",
    "# Create a separate dataframe consisting only of the features i.e independent attributes\n",
    "X = Medical.drop(labels= ['Class'] , axis = 1)\n",
    "y = Medical[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the features into z scores as we do not know what units / scales were used and store them in new dataframe\n",
    "# It is always adviced to scale numeric attributes in models that calculate distances.\n",
    "\n",
    "XScaled  = X.apply(zscore)  # convert all attributes to Z scale \n",
    "\n",
    "XScaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.B\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q4.C\n",
    "NNH = KNeighborsClassifier(n_neighbors= 3)\n",
    "# Call Nearest Neighbour algorithm\n",
    "NNH.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the response for train\n",
    "# predict the response\n",
    "y_pred_train = NNH.predict(X_train)\n",
    "\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_train, y_pred_train))\n",
    "print('Recall Score:',recall_score(y_train, y_pred_train,average=\"weighted\"))\n",
    "print('Precision Score:',precision_score(y_train, y_pred_train,average=\"weighted\")) #because three categories\n",
    "print('F1 Score:',f1_score(y_train, y_pred_train,average=\"weighted\"))\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred_train)\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [\"0\", \"1\",\"2\"], \n",
    "                     columns = [\"0\", \"1\",\"2\"])\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print ('Classification Report : ')\n",
    "print (classification_report(y_train, y_pred_train, target_names=[\"0\", \"1\",\"2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the response for test \n",
    "y_pred = NNH.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred,average=\"weighted\"))\n",
    "print('Precision Score:',precision_score(y_test, y_pred,average=\"weighted\")) #because three categories\n",
    "print('F1 Score:',f1_score(y_test, y_pred,average=\"weighted\"))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [\"0\", \"1\",\"2\"], \n",
    "                     columns = [\"0\", \"1\",\"2\"])\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print ('Classification Report : ')\n",
    "print (classification_report(y_test, y_pred, target_names=[\"0\", \"1\",\"2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Performance Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.A\n",
    "from sklearn.model_selection import GridSearchCV #this will help us in finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p, weights = weights)\n",
    "\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10,scoring = 'accuracy')\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 8,weights =  'distance',leaf_size=1,p=2)\n",
    "knn.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the response\n",
    "y_pred = knn.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred,average=\"weighted\"))\n",
    "print('Precision Score:',precision_score(y_test, y_pred,average=\"weighted\"))\n",
    "print('F1 Score:',f1_score(y_test, y_pred,average=\"weighted\"))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [\"0\", \"1\",\"2\"], \n",
    "                     columns = [\"0\", \"1\",\"2\"])\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print ('Classification Report : ')\n",
    "print (classification_report(y_test, y_pred, target_names=[\"0\", \"1\",\"2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightly_identified(actual, predictions):\n",
    "    a = pd.DataFrame()  # empty dataframe\n",
    "\n",
    "    for i in np.sort(actual.unique()):\n",
    "        try:  # because we get a key error if we use value_counts()[i] when the given class i does not exist in the value_count() array\n",
    "            a.loc[i,'total_instances'] = len(actual[actual == i]) # value count of a given class i\n",
    "            a.loc[i, 'identified_right'] = pd.Series(predictions[actual == i]).value_counts()[i] # number of instances where a given class i was identified correctly\n",
    "            a.loc[i, '%'] = round(100*pd.Series(predictions[actual == i]).value_counts()[i]/len(actual[actual == i]), 1)  # percentage of predictions that were correct for a given class\n",
    "        except:  # if the key error discussed above occurs, assign the percentage = 0\n",
    "            a.loc[i, '%'] = 0.0\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vs_actual_lr = rightly_identified(y_test, y_pred)\n",
    "pred_vs_actual_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an overall improvement in Precision, accuracy, recall and F1 score for the model by 2% at an overall level. But at an individual class level, \"1\" class performance improved drastically in terms of all the above metrics. There is a +10% improvement in precsion, recall, f1-score for \"1\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below parameters helped in improving the model performance:\n",
    "    1. leaf_size: 1 earlier it was 30\n",
    "    2. p: 2\n",
    "    3. n_neighbors: 8 earlier it was 3\n",
    "    4. weights: distance earlier it was uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: scaling was also tried but it didn't increase any score hence removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and test set in 80:20 ratio\n",
    "XScaled  = X.apply(zscore)  # convert all attributes to Z scale \n",
    "\n",
    "XScaled.describe()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XScaled, y, test_size=0.20, random_state=1)\n",
    "\n",
    "NNH = KNeighborsClassifier(n_neighbors= 5 , weights = 'distance' )\n",
    "\n",
    "# Call Nearest Neighbour algorithm\n",
    "\n",
    "NNH.fit(X_train, y_train)\n",
    "\n",
    "# For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n",
    "# be assigned to the test data point\n",
    "\n",
    "predicted_labels = NNH.predict(X_test)\n",
    "NNH.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, predicted_labels))\n",
    "print('Recall Score:',recall_score(y_test, predicted_labels,average=\"weighted\"))\n",
    "print('Precision Score:',precision_score(y_test, predicted_labels,average=\"weighted\"))\n",
    "print('F1 Score:',f1_score(y_test, predicted_labels,average=\"weighted\"))\n",
    "\n",
    "cm = confusion_matrix(y_test,predicted_labels)\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [\"0\", \"1\",\"2\"], \n",
    "                     columns = [\"0\", \"1\",\"2\"])\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print ('Classification Report : ')\n",
    "print (classification_report(y_test, predicted_labels, target_names=[\"0\", \"1\",\"2\"]))\n",
    "\n",
    "pred_vs_actual_lr = rightly_identified(y_test, predicted_labels)\n",
    "pred_vs_actual_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.A\n",
    "Data1 = pd.read_csv('Part2+-+Data1.csv') #reading the dataset\n",
    "Data2 = pd.read_csv('Part2+-Data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.B\n",
    "#shape\n",
    "print('Shape of Data1:', Data1.shape)\n",
    "print('Shape of Data2:', Data2.shape)\n",
    "\n",
    "#columns\n",
    "print('Columns of Data1:', Data1.columns)\n",
    "print('Columns of Data2:', Data2.columns)\n",
    "\n",
    "#DataTypes\n",
    "print('DataTypes of Data1:', Data1.info())\n",
    "print('DataTypes of Data2:', Data2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.C\n",
    "result = pd.merge(Data1, Data2, on=[\"ID\"]) #inner join basis id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.D\n",
    "convert_dict = {'CreditCard' : object,\n",
    "                'InternetBanking': object,\n",
    "                'FixedDepositAccount': object,\n",
    "                'Security': object, \n",
    "                'Level': object,\n",
    "                'HiddenScore': object\n",
    "               }\n",
    "  \n",
    "result = result.astype(convert_dict)\n",
    "print(result.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.A\n",
    "result['LoanOnCard'].hist()\n",
    "result.groupby(\"LoanOnCard\").agg({'LoanOnCard': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. The above graph shows that the dataset is higly imbalanced with 90% of the accounts haven't taken loan on the card.\n",
    "    2. Also, 20 accounts don't have any entry for 'LoanOnCard' feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q2.B\n",
    "# result.isnull().sum()\n",
    "percent_missing = result.isnull().sum() * 100 / len(result)\n",
    "print(percent_missing)\n",
    "\n",
    "\n",
    "result['LoanOnCard'].fillna(result['LoanOnCard'].mode()[0], inplace=True)\n",
    "print(result.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.40 % of the data is missing for \"LoanOnCard\" feature and for the other features there are no nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q5.C\n",
    "list_columns= ['HiddenScore','Level','Security','FixedDepositAccount','InternetBanking','CreditCard']\n",
    "for i in list_columns:\n",
    "    print(i)\n",
    "    print(result[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "There are no unexpected values which needs treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NAN\" in result.values:\n",
    "        print('Element exists in Dataframe')\n",
    "        \n",
    "if \"?\" in result.values:\n",
    "        print('Element exists in Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in result.columns:\n",
    "    print(i)\n",
    "    print(result[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Data Preparation and model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(result['FixedDepositAccount'],label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(result['Security'],label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Although above variables have low variance but let's keep it as already the number of variables are low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.corr()\n",
    "#Basis this we can remove either Age or Customer Since feature because they are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.A\n",
    "# Create a separate dataframe consisting only of the features i.e independent attributes\n",
    "X = result.drop(labels= ['ID','ZipCode','LoanOnCard','CustomerSince'] , axis = 1)\n",
    "#CustomerSince dropped due to high  multicollinarity with age feature\n",
    "y = result[\"LoanOnCard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.B\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.C\n",
    "#Logistic Regression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit the model on train\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(X_train, y_train)\n",
    "#predict on test\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "\n",
    "coef_df = pd.DataFrame(model.coef_)\n",
    "coef_df['intercept'] = model.intercept_\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "# roc curve for logistic regression model with optimal threshold\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--',label='No Skill')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G-Mean = sqrt(Sensitivity * Specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "yhat=model.predict_proba(X_test)[:,1]\n",
    "# calculate roc curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, yhat)\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "# no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "# plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Logistic')\n",
    "plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the response\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print(\"TEST\")\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC-ROC: {}\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "    1. Model is able to predict very well who will not take loan on card (1100 out of 1113 in test data)\n",
    "    2. Model looses on accounts who will take loan. It's not able to predict 1's very well. And, hence the recall is also low because False negative is high in number.\n",
    "    3. F1 is harmonic mean of Precision and recall. It's value is also low because recall is low.\n",
    "    4. Precision and accuracy of the model is good which might be because of good True positives and low False positives.\n",
    "    5. The above graphs help on deciding the threshold basis the metric which we consider as important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of outliers and applying L2 regularisation on standardised data to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.boxplot(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows lot of outliers in Mortgage feature which might affect the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Lot of outliers-Let's remove these to see the impact,\n",
    "#Implementing the above steps will free the final logistic regression model from extremely misclassified data points.\n",
    "\n",
    "\n",
    "# Create a separate dataframe consisting only of the features i.e independent attributes\n",
    "X = result.drop(labels= ['ID','ZipCode','LoanOnCard','CustomerSince'] , axis = 1)\n",
    "y = result[\"LoanOnCard\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# to scale the data so as to bring everything on same scale\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "print(X_train_std.shape)\n",
    "print(X_test_std.shape)\n",
    "\n",
    "\n",
    "param_grid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"],\"solver\": [\"saga\",\"liblinear\",\"warn\"]}\n",
    "\n",
    "classifier = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "logreg_cv = GridSearchCV(classifier, param_grid, cv=3, scoring='roc_auc').fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"roc_auc score :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "classifier = LogisticRegression(random_state=42, n_jobs=-1, **params).fit(X_train_std, y_train)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_std)\n",
    "y_pred_proba = classifier.predict_proba(X_test_std)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np. unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST\")\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC-ROC: {}\".format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector = list(classifier.coef_[0])\n",
    "weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.dot(X_train_std, weight_vector)\n",
    "y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]\n",
    "len(y_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.kdeplot(y_dist)\n",
    "plt.xlabel(\"Distance * Y-class\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.percentile(y_dist, 5) #remove above and below 5 percentile value\n",
    "print(\"Threshold Val: \", val)\n",
    "\n",
    "y_train[(y_dist < val)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std_new = X_train_std[(~(y_dist < val))]\n",
    "y_train_new = y_train[(~(y_dist < val))]\n",
    "print(X_train_std_new.shape)\n",
    "print(y_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's train the same model on new data after removal of outliers\n",
    "params = {'C': 0.1, 'penalty': 'l2','solver':'liblinear'}\n",
    "classifier1 = LogisticRegression(random_state=42, n_jobs=-1, **params).fit(X_train_std_new, y_train_new)\n",
    "classifier1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier1.predict(X_test_std)\n",
    "y_pred_proba = classifier1.predict_proba(X_test_std)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"TEST\")\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC-ROC: {}\".format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model performance without L2, OUTLIER treatment and standardisation.\n",
    "\n",
    "TEST\n",
    "1. Precision: 0.8586956521739131\n",
    "2. Recall: 0.5895522388059702\n",
    "3. F1 Score: 0.6991150442477877\n",
    "4. Accuracy: 0.9456\n",
    "5. AUC-ROC: 0.7889517466431285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "\n",
    "By applying l2 regularisation we have also considered the fact that overfitting must be regularised and removal of outliers have helped in increasing the  recall from 0.58 to 0.76 and F1_Score  from 0.69 to 0.75 compared to without any treatment.\n",
    "\n",
    "Recall is important for this problem as we don't want to miss out on \"1\" for marketing.\n",
    "\n",
    "From confusion matrix we can say True positives have increased while false negative has decreased \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.E\n",
    "# Shuffle the Dataset.\n",
    "shuffled_df = result.sample(frac=1,random_state=4)\n",
    "\n",
    "# Put all the \"1\" class in a separate dataset.\n",
    "Loan = result.loc[result['LoanOnCard'] == 1]\n",
    "\n",
    "#Randomly select 480 observations from the 0 (majority class)\n",
    "No_Loan = shuffled_df.loc[result['LoanOnCard'] == 0].sample(n=480,random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "normalized_df = pd.concat([Loan, No_Loan])\n",
    "\n",
    "#plot the dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('LoanOnCard', data=normalized_df)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.groupby(\"LoanOnCard\").agg({'LoanOnCard': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.F\n",
    "X = normalized_df.drop(labels= ['ID','ZipCode','LoanOnCard','CustomerSince'] , axis = 1)\n",
    "y = normalized_df[\"LoanOnCard\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# to scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "print(X_train_std.shape)\n",
    "print(X_test_std.shape)\n",
    "\n",
    "# Training previous model on balanced data\n",
    "params = {'C': 0.1, 'penalty': 'l2','solver':'liblinear'}\n",
    "classifier1 = LogisticRegression(random_state=42, n_jobs=-1, **params).fit(X_train_std, y_train)\n",
    "classifier1\n",
    "\n",
    "\n",
    "y_pred = classifier1.predict(X_test_std)\n",
    "y_pred_proba = classifier1.predict_proba(X_test_std)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.G\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC_ROC: {}\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "When we fitted the balanced dataset on the same model, it performed well on the below metrics\n",
    "\n",
    "    1. Precision: 0.73 to 0.81\n",
    "    2. Recall: 0.76 to 0.88\n",
    "    3. F1 Score: 0.75 to 0.84\n",
    "    4. From confusion matrix we can say false negative and false positives have decreased\n",
    "    \n",
    "    Note: Please find below previouus model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### this is the perfomance of model on test data after removal of outliers\n",
    "    1. Precision: 0.7357142857142858\n",
    "    2. Recall: 0.7686567164179104\n",
    "    3. F1 Score: 0.7518248175182483\n",
    "    4. Accuracy: 0.9456\n",
    "    5. AUC-ROC: 0.9671066709463436"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Performance Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe consisting only of the features i.e independent attributes\n",
    "X = result.drop(labels= ['ID','ZipCode','LoanOnCard','CustomerSince'] , axis = 1)\n",
    "#CustomerSince dropped due to high  multicollinarity\n",
    "y = result[\"LoanOnCard\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.A \n",
    "\n",
    "#SVM\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.025, C=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grid = (np.column_stack([y_test, y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 26)\n",
    "\n",
    "pd.crosstab(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the response\n",
    "y_pred = clf.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('f1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "\n",
    "NNH = KNeighborsClassifier(n_neighbors= 1 , weights = 'distance' )\n",
    "# Call Nearest Neighbour algorithm\n",
    "\n",
    "NNH.fit(X_train, y_train)\n",
    "\n",
    "# For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n",
    "# be assigned to the test data point\n",
    "\n",
    "predicted_labels = NNH.predict(X_test)\n",
    "NNH.score(X_test, y_test)\n",
    "\n",
    "# predict the response\n",
    "y_pred = NNH.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('f1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if we standardize it and then work\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "#Increase C to avoid overfitting\n",
    "svc = svm.SVC(C=1000)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('F1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trying snother kernel\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Building a Support Vector Machine on train data\n",
    "svc_model = SVC(C= .1, kernel='linear', gamma= 1)\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "prediction = svc_model .predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(prediction,y_test))\n",
    "\n",
    "# predict the response\n",
    "y_pred = svc_model.predict(X_test_scaled)\n",
    "\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('F1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is bad compared to the above model basis f1, recall and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best model from SVM is when C=1000 and Kernel = rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now let's tune KNN\n",
    "# KNN - choosing the K value\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList = list(range(2,20))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold accuracy scores\n",
    "ac_scores = []\n",
    "\n",
    "# perform accuracy metrics for values from 1,3,5....19\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "    # predict the response\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # evaluate accuracy\n",
    "    scores = accuracy_score(y_test, y_pred)\n",
    "    ac_scores.append(scores)\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in ac_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN - Model using the best parameters form above\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=5,weights='distance')\n",
    "KNN.fit(X_train, y_train)\n",
    "# predict the response\n",
    "y_pred = KNN.predict(X_test)\n",
    "# evaluate accuracy\n",
    "KNN_Accuracy=accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy using KNN : \", KNN_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the response\n",
    "y_pred = KNN.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('F1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's standardise this and see\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.info()\n",
    "X = X.astype(float)\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XScaled  = X.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and test set in 75:25 ratio\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XScaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#KNN\n",
    "# KNN - choosing the K value after standardising\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList = list(range(2,20))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold accuracy scores\n",
    "ac_scores = []\n",
    "\n",
    "# perform accuracy metrics for values from 1,3,5....19\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "    # predict the response\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # evaluate accuracy\n",
    "    scores = accuracy_score(y_test, y_pred)\n",
    "    ac_scores.append(scores)\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in ac_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NNH = KNeighborsClassifier(n_neighbors= 3 , weights = 'distance' )\n",
    "# Call Nearest Neighbour algorithm\n",
    "\n",
    "NNH.fit(X_train, y_train)\n",
    "\n",
    "# For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n",
    "# be assigned to the test data point\n",
    "\n",
    "predicted_labels = NNH.predict(X_test)\n",
    "NNH.score(X_test, y_test)\n",
    "\n",
    "# predict the response\n",
    "y_pred = NNH.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('F1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights \n",
    "\n",
    "Finalising the SVM model (SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
    "    shrinking=True, tol=0.001, verbose=False)) built on scaled data. Scaling has drastically improved the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score: 0.9784\n",
    "Recall Score: 0.917910447761194\n",
    "Precision Score: 0.8848920863309353\n",
    "F1 Score: 0.9010989010989011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.C\n",
    "#if we standardize it and then work\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# predict the response\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "# evaluate accuracy\n",
    "print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
    "print('Recall Score:',recall_score(y_test, y_pred))\n",
    "print('Precision Score:',precision_score(y_test, y_pred))\n",
    "print('f1 Score:',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm=metrics.confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.D\n",
    "1. Improvement in terms of recall, accuracy,precision and f1 score has been observed. \n",
    "2. Also, true positive has increased and false negative&false positives has further decreased comparitively\n",
    "\n",
    "Old Base SVM results for comparison\n",
    "\n",
    "#SVM performance\n",
    "    1. Accuracy Score: 0.928\n",
    "    2. Recall Score: 0.47761194029850745\n",
    "    3. Precision Score: 0.7619047619047619\n",
    "    4. f1 Score: 0.58\n",
    "    \n",
    "Old Base KNN results for comparison \n",
    "\n",
    "#KNN performance\n",
    "    1. Accuracy Score: 0.9016\n",
    "    2. Recall Score: 0.4701492537313433\n",
    "    3. Precision Score: 0.5478260869565217\n",
    "    4. f1 Score: 0.5060240963855422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
